---
title: "Dada2 processing"
output: html_document

params:
  projectname:
    label: "Name of the report"
    value: "MockTest"
    input: text
  dirRaw:
    label: "Working directory"
    value: "../raw"
    input: text
  dirOut:
    label: "Output directory"
    value: "../"
    input: text
  forwTrim:
    label: "Trimming forward position"
    value: 240
    input: numeric
  revrTrim:
    label: "Trimming reverse position"
    value: 210
    input: numeric
  minreads:
    label: "Minimum reads number"
    input: numeric
    value: 25000
  lenASVlow:
    label: "Lower range of ASV length"
    value: 247
    input: numeric
  lenASVup:
    label: "Upper range of ASV length"
    value: 257
    input: numeric

---

```{r setup1, echo = FALSE}

knitr::opts_chunk$set(echo = FALSE, fig.align="center")
```


```{r reproduce1}

## Change to FALSE if you ran the analysis and only want to produce the report again
firstRun1 <- FALSE
```


```{r paramDirs1, include = FALSE, child = "conditions/0dirCheck.Rmd"}

```


```{r reportDir1, include = FALSE}

## Filtered folder
fltDir <- file.path(reportDir, "filtered")
inferDir <- file.path(reportDir,"inference") 
ifelse(!dir.exists(inferDir), dir.create(inferDir), FALSE)
errorDir <- file.path(reportDir, "errorRates")
ifelse(!dir.exists(errorDir), dir.create(errorDir), FALSE)
```


```{r trimmingParams}

## Condition if pre-check file exists
finsamfile <- file.path(file.path(reportDir,"metafile.csv"))

## Standalone script load in.
## Check file exists
if (file.exists(finsamfile)) {
  ## Set file to variable
  expriFile <- read.csv(finsamfile)
  expriFile$X <- NULL
} else {
  print ("Missing processing file to continue further. Please go back one step.")
  knitr::knit_exit()
} 

## Trimming forward value
if (!is.null(params$trimForward)){
  cutForw <- params$trimForward
} else {
  cutForw <- params$forwTrim
}

## Trimming reverse value
if (!is.null(params$trimReverse)){
  cutRerv <- params$trimReverse
} else {
  cutRerv <- params$revrTrim
}

## Turns on standalone settings for min reads
if (is.null(params$minReads)) {
  min_reads <- params$minreads
} else {
  min_reads <- minReads
}

## Check primers from main script
## If either are not null
if (!is.null(params$primerFor) | !is.null(params$primerFor)){

  ## If both primers are not empty
  if (params$primerFor != '' && params$primerRev != ''){

    ## Both primers found on main script
    FWD <- primerForward
    REV <- primerReverse
    primerFound <- TRUE
  } else {

    ## Main script has no primers
    primerFound <- FALSE
  }
} else {

  ## No main script
  FWD <- params$priFor
  REV <- params$priRev

  if (FWD != '' && REV != ''){

    ## Standalone primers
    primerFound <- TRUE
  } else {

    ## No stanalone primers
    primerFound <- FALSE
  }
}

## Turns on standalone settings for amplicon length
if (is.null(params$asvLenLow)) {

  asvLeft <- params$lenASVlow
} else {

  asvLeft <- asvLow
}

## Turns on standalone settings for amplicon length
if (is.null(params$asvLenUp)) {

  asvRight <- params$lenASVup
} else {

  asvRight <- asvUp
}
```


```{r lib1, include = FALSE}

library("dada2")
library("tidyverse")
library(ShortRead)
library(Biostrings)
start_time <- Sys.time()
```


```{r trimRaws, eval = firstRun1, include = FALSE, warning = FALSE}

## Only analyse found files
expriFound <- expriFile[expriFile$Status=="Found",]
readtrackTrim <- data.frame(matrix(nrow = nrow(expriFound), ncol = 1))

if (dir.exists(file.path(reportDir, "cutadapted"))) {
  pathRaw <- file.path(reportDir, "cutadapted")
}  

## Working on sample
for (i in 1:nrow(expriFound)){
    
  ## Sample names
  sampName <- expriFound$Samples[i]
  
  ## Identify forward and reverse reads
  fnF <- intersect(list.files(pathRaw, pattern = paste0(sampName,"_"), full.names = TRUE), list.files(pathRaw, pattern = "_R1", full.names = TRUE))
  fnR <- intersect(list.files(pathRaw, pattern = paste0(sampName,"_"), full.names = TRUE), list.files(pathRaw, pattern = "_R2", full.names = TRUE))  
  
  ## Name of filtered reads
  filtF <- file.path(fltDir, paste0(sampName, "_F_filt.fastq.gz"))
  filtR <- file.path(fltDir, paste0(sampName, "_R_filt.fastq.gz"))
  ## names function to set name of an object
  names(filtF) <- sampName
  names(filtR) <- sampName
    
  if ((cutForw=="None") | (cutRerv=="None")){

  ## Raw read in and out after trimming
  out <- filterAndTrim(fnF, filtF, fnR, filtR,
                   maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                   compress=TRUE, multithread=TRUE)

  ## Save first line
  readtrackTrim[i,1] <- out[2]
  } else {

  out <- filterAndTrim(fnF, filtF, fnR, filtR, truncLen=c(cutForw,cutRerv),
                   maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                   compress=TRUE, multithread=TRUE)

  ## Save first line
  readtrackTrim[i,1] <- out[2]
  }
}

## For checking

colnames(readtrackTrim) <- c("filtered")
readtrackTrim$Samples <- expriFound$Samples

readsFlow <- read.csv(file.path(reportDir,"readtrackRawN.csv"))
readsFlow$X <- NULL
readsFlow <- merge(readsFlow, readtrackTrim, by="Samples")

write.csv(readsFlow, file.path(reportDir,"readtrack.csv"))

```


```{r trimmedCheck}

if (!file.exists(file.path(reportDir,"readtrack.csv")) ){
  print("Please trim raw files to process further")
  knitr::knit_exit()
}
```


## Reads filtering


Filtering out low-quality sequencing reads and trimming the reads.
Often the reverse read's quality drops significantly from the last 90 nucleotides.
The final length of the forward and reverse reads are `r cutForw` and `r cutRerv` nucleotides, respectively.
After trimming, the standard filtering setting was used with the enforcement of maximum of 2 expected errors per-read.  
Expected errors (EE) is calculated from the nominal definition of the quality score: EE = sum(10^(-Q/10)).
Any contaminated phiX genomes were filtered out.


```{r afterFilterFig, eval = firstRun1, include = FALSE}

filtFR <- list.files(fltDir, pattern = "filt", full.names = TRUE)
readsFilt <- plotQualityProfile(filtFR[1:2])

ggsave(file.path(reportDir,"filteredReads.png"), readsFilt, device="png")

```

```{r filteredRaw, fig.cap = "Trimmed first pair"}

knitr::include_graphics(file.path(reportDir, "filteredReads.png"))

```


```{r readsCheck}

## Condition to check if trimming removed most of the reads
readtrack <- read.csv(file.path(reportDir,"readtrack.csv"))
rawSum <- sum(readtrack$raw)
filteredSum <- sum(readtrack$filtered)
if (filteredSum / rawSum < 0.1) {
  print ("The sum of filtered reads are less than 10%. Recheck trimming length!")
  knitr::knit_exit()
}
```


## Error rates 


Different amplicon dataset has a different set of error rates.
DADA2 method use relies on a parameterized model of substitution errors to distinguish sequencing errors from real biological variation.
The protocol uses a form of unsupervised learning in which sample inference is alternated with parameter estimation until both are jointly consistent.


```{r filteredFiles, eval = firstRun1}

## Find forward and reverse filtered files
filtFs <- list.files(fltDir ,pattern="_F_filt.fastq.gz", full.names = TRUE)
filtRs <- list.files(fltDir, pattern="_R_filt.fastq.gz", full.names = TRUE)
## Extract sample names from folder
sampleNames <- sapply(strsplit(basename(filtFs), "_"), `[`, 1)
## names function to set name of an object
names(filtFs) <- paste0(sampleNames,"_")
names(filtRs) <- paste0(sampleNames,"_")
## Load in reads tracking file
readstrack <- read.csv(file.path(reportDir,"readtrack.csv"), row.names = 1)
trimmedSamples <- readstrack$Samples
## Keep intersect samples
finSampleNames <- intersect(sampleNames, trimmedSamples)
```


```{r dadaFunc}

dada2analysis <- function(trimmedNames, poolNumb) {
  <<sampleDetect>>
  <<errorRates>>
  <<inference>>
}
```


```{r sampleDetect, eval = FALSE}

pat <- paste(trimmedNames, collapse = '|')
ffiltered <- intersect(list.files(fltDir, pattern=pat, full.names=TRUE), list.files(fltDir, pattern="F_filt", full.names=TRUE))
rfiltered <- intersect(list.files(fltDir, pattern=pat, full.names=TRUE), list.files(fltDir, pattern="R_filt", full.names=TRUE))
## Extract sample names from folder
sampleNames <- sapply(strsplit(basename(ffiltered), "_"), `[`, 1)
## names function to set name of an object
names(ffiltered) <- paste0(sampleNames,"_")
names(rfiltered) <- paste0(sampleNames,"_")
```


```{r errorRates, eval = FALSE, results = 'hide', message = FALSE, warning = FALSE}

## Set seed 
set.seed(100)
errF <- learnErrors(ffiltered, nbases = 1e8, multithread=TRUE, randomize=TRUE)
errR <- learnErrors(rfiltered, nbases = 1e8, multithread=TRUE, randomize=TRUE)
## Save error rates
saveRDS(errF, file.path(errorDir, paste0(poolNumb, "_errorforwd.RDS")))
saveRDS(errR, file.path(errorDir, paste0(poolNumb, "_errorrevse.RDS")))
errForward <- plotErrors(errF, nominalQ=TRUE) 
ggsave(file.path(errorDir, paste0(poolNumb, "_forwdErrors.png")), errForward , device="png")
errReverse <- plotErrors(errR, nominalQ=TRUE)
ggsave(file.path(errorDir, paste0(poolNumb, "_revrdErrors.png")), errReverse , device="png")
```


```{r inference, eval = FALSE}

## Load error rates
errF <- readRDS(file.path(errorDir, paste0(poolNumb, "_errorforwd.RDS")))
errR <- readRDS(file.path(errorDir, paste0(poolNumb, "_errorrevse.RDS")))
## Create vector to store data
dds <- vector("list", length(trimmedNames))
names(dds) <- trimmedNames
for(sam in trimmedNames) {
  
  ## Remove duplicate reads
  derepF <- derepFastq(ffiltered[[sam]])
  derepR <- derepFastq(rfiltered[[sam]])
  
  ## Inference using error rate
  dadaFs <- dada(derepF, err=errF, multithread=TRUE)
  saveRDS(dadaFs, file.path(inferDir, paste0(sam,"_inferFs.RDS")))
  dadaRs <- dada(derepR, err=errR, multithread=TRUE)
  saveRDS(dadaRs, file.path(inferDir, paste0(sam,"_inferRs.RDS")))
  ## Merge pairs
  dds[[sam]] <- mergePairs(dadaFs, filtFs[[sam]], dadaRs, filtRs[[sam]], verbose=TRUE)
#  dds[[sam]] <- mergePairs(dadaFs, filtFs[[sam]], dadaRs, filtRs[[sam]], minOverlap = 12, verbose=TRUE)
  ##dds[[sam]] <- mergePairs(dadaFs, filtFs[[sam]], dadaRs, filtRs[[sam]], verbose=TRUE, justConcatenate=TRUE)
}
saveRDS(dds, file.path(inferDir, paste0(poolNumb, "_dds.RDS")))
# Construct sequence table and write to disk
seqtab <- makeSequenceTable(dds)
seqtabFile <- file.path(errorDir, paste0(poolNumb, "_seqtab.RDS"))
saveRDS(seqtab, seqtabFile)
```


```{r trimmedFiles, eval = FALSE}

## Load back metafile
expriFile <- read.csv(finsamfile)
expriFile$X <- NULL
## Takeout filtered samples
analysisFiltered <- expriFile[expriFile$Samples %in% finSampleNames,]
expriFound <- analysisFiltered[analysisFiltered$Status=="Found",]
```

```{r dada2Run, eval = firstRun1, include = FALSE, results = 'hide', message = FALSE, warning = FALSE}

<<trimmedFiles>>
<<dadaFunc>>
if (("Pool" %in% colnames(expriFound)) && !("" %in% unique(expriFound$Pool))){
  ## Pool analysis. Only works if column is filled
  for (u in unique(expriFound$Pool)){
    poolRun <- expriFound[expriFound$Pool==u,"Samples"]
    poolRun <- paste0(poolRun,"_")
    dada2analysis (poolRun, u)
  }
} else {
  ## No pool, analyse samples as one
  noPoolSamples <- analysisFiltered$Samples
  noPoolSamples <- paste0(noPoolSamples,"_")
  dada2analysis (noPoolSamples, "Pool")
}
```


```{r combineRuns, eval = firstRun1, message = FALSE, warning = FALSE}

<<trimmedFiles>>
## Incase of multiple pools, reverse sequences incase sequences are reversed
if (("Pool" %in% colnames(expriFound)) && !("" %in% unique(expriFound$Pool))){
  ## All pool names
  allPools <- data.frame(expriFound %>% group_by(Pool) %>% count(Pool) %>% arrange(n) )
  ## First pool as base
  baseSeqtabName <- allPools[["Pool"]][1]
  baseSeqtabPath <- file.path(errorDir, paste0(baseSeqtabName, "_seqtab.RDS"))
  
  ## Check file exists
  if (file.exists(baseSeqtabPath)){
    seqtabBase <- readRDS(baseSeqtabPath)
  } else {
    cat("Missing Seqtab file. Please recheck!")
  }
  
  ## Condition for single pool
  if (nrow(allPools) > 1) {
    ## Start from the second pool onward
    for (p in 2:nrow(allPools)){
    
      checkSeqtab <- allPools[["Pool"]][p]
      checkSeqtabPath <- file.path(errorDir, paste0(checkSeqtab, "_seqtab.RDS"))
    
      if (file.exists(checkSeqtabPath)){
    
        currentSeqtab <- readRDS(checkSeqtabPath)
        ## Moves on when blank file is loaded
        if (ncol(currentSeqtab) == 0){
          next
        } else {
 
          ## Merge sequence table in original orientation
          mergetab <- mergeSequenceTables(seqtabBase, currentSeqtab)
          ## Reverse sequences and merge
          currentSeqtab2 <- currentSeqtab
          colnames(currentSeqtab2) <- dada2:::rc(colnames(currentSeqtab))
          mergetab2 <- mergeSequenceTables(seqtabBase, currentSeqtab2)
          ## Check which orientation has less sequences
          if (ncol(mergetab) < ncol(mergetab2)){
            seqtabBase <- mergetab
          } else {
            seqtabBase <- mergetab2
          }
        }
      } else {
        cat("Missing Seqtab file. Please recheck!")
      }
    } 
  }
## Condition for no pool
} else {
  baseSeqtabPath <- file.path(errorDir, paste0("Pool", "_seqtab.RDS"))
  if (file.exists(baseSeqtabPath)){
    seqtabBase <- readRDS(baseSeqtabPath)
  }
}
## listSeqTab <- list.files(path, pattern = "seqtab", full.name=TRUE)
## mergetab <- mergeSequenceTables(tables=listSeqTab)
saveRDS(seqtabBase, file.path(reportDir,"seqtab.RDS"))
```


```{r forrevErr, eval = FALSE, fig.cap = "Error rate for forward reads"}

<<trimmedFiles>>
if (("Pool" %in% colnames(expriFound)) && !("" %in% unique(expriFound$Pool))){
  allPools <- data.frame(expriFound %>% group_by(Pool) %>% count(Pool) %>% arrange(n) )
  poolName <- allPools[["Pool"]][1]
  knitr::include_graphics(file.path(errorDir, paste0(poolName, "_forwdErrors.png")))
} else {
  knitr::include_graphics(file.path(errorDir, paste0("Pool", "_forwdErrors.png")))
}
```


```{r revErr, eval = FALSE, fig.cap = "Error rate for the reverse reads"}

<<trimmedFiles>>
if (("Pool" %in% colnames(expriFound)) && !("" %in% unique(expriFound$Pool))){
  allPools <- data.frame(expriFound %>% group_by(Pool) %>% count(Pool) %>% arrange(n) )
  poolName <- allPools[["Pool"]][1]
  knitr::include_graphics(file.path(errorDir, paste0(poolName, "_revrdErrors.png")))
} else {
  knitr::include_graphics(file.path(errorDir, paste0("Pool", "_revrdErrors.png")))
}
```


## Preprocessing


At this stage the reads are dereplicated to remove identical sequences to reduce computation time.
They are inferenced and merged together to obtain the full denoised sequences.
Sequences with forward and reverse reads overlaping by at least 12 bases, and are identical to each other in the overlap regions are aligned together.    


```{r readLengths, fig.cap = "Distribution of sequence lengths"}

## Load in seqtab
seqstab <- readRDS(file.path(reportDir,"seqtab.RDS"))
### Seqtab is a matrix
## Histogram of read lengths
hist(nchar(getSequences(seqstab)), main=NULL, xlab="Sequence lengths", ylab="Number of Sequence/ASVs")
```

Due to non-specific priming, some sequences were much longer or shorter than expected.
These non-target-length sequences outside of the amplicon range `r asvLeft` and `r asvRight` were removed.
Chimeric sequences were identified and removed if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r uniqASVs, eval = firstRun1, include = FALSE}

## Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqstab, method="consensus", multithread=TRUE, verbose=TRUE)

seqstab.nochim <- seqtab.nochim[,nchar(colnames(seqtab.nochim)) %in% asvLeft:asvRight]

## Rename sample.names back
rownames(seqstab.nochim) <- gsub("_$","",rownames(seqstab.nochim))
seqtabnochimFile <- file.path(reportDir,"seqtab_nochim.RDS")
saveRDS(seqstab.nochim, seqtabnochimFile)
```

```{r removeLongSeq, fig.cap = "Distribution of sequence lengths after length removal"}

seqstab.nochim <- readRDS(file.path(reportDir,"seqtab_nochim.RDS"))
hist(nchar(getSequences(seqstab.nochim)), main=NULL, xlab="Sequence lengths", ylab="Number of Unique Sequence/ASVs")
```

```{r tracking_reads}

## Load in reads tracking file
readstrack <- read.csv(file.path(reportDir,"readtrack.csv"), row.names = 1)

## Turns samples to rowname
rownames(readstrack) <- readstrack$Samples
readstrack$Samples <- NULL
## Combine values
#track <- cbind(readstrack, rowSums(seqstab), rowSums(seqstab.nochim))
boundSeqstab <- cbind(rowSums(seqstab), rowSums(seqstab.nochim))
rownames(boundSeqstab) <- gsub("_$","",rownames(boundSeqstab))
track <- merge(readstrack, boundSeqstab, by="row.names")
## Rename columns
colnames(track) <- c("Samples","raw", "Nfiltered", "filtered", "merged", "nonchim")
track <- track %>% filter(nonchim > 0)
readsTracking <- file.path(reportDir,"reads_tracking.csv")
write.csv(track, readsTracking)
## Merge sample with reads tracking
sampTrack <- merge(expriFile, track[,c("Samples","nonchim")], by="Samples")
sampTrack$Status <- NULL
sampTrackReads <- file.path(reportDir,"finSamples_reads.csv")
write.csv(sampTrack, sampTrackReads)
percSeq <- round((sum(seqstab)-sum(seqstab.nochim))/sum(seqstab)*100, digits = 2)
```


For this run, the removed chimeric and non-target-length sequences were about `r percSeq` % in total.  
The final reads will be used for taxonomic typing and can be downloaded: `r xfun::embed_file(file.path(reportDir,"seqtab_nochim.RDS"), text = "here !")`


```{r libSizeCheck}

## Condition incase of analysis not from the beginning
readsTracking <- file.path(reportDir,"finSamples_reads.csv")
sampTrack <- read.csv(readsTracking)
sampTrack$X <- NULL
names(sampTrack)[names(sampTrack) == "Row.names"] <- "Samples"
## Check if specific columns exist 
colTitles <- c("Sample_Type")
if (any(colTitles %in% colnames(sampTrack))) {
  
  libSizFig = TRUE
} else {
  libSizFig = FALSE
  write.csv(sampTrack, file.path(reportDir, "filterdExp.csv"))
 
}
```


```{r rawLib, child = if (libSizFig) "conditions/1libSize.Rmd" }

```


```{r sequencetabNoChimera}

filterdExp <- read.csv(file.path(reportDir, "filterdExp.csv"))
filterdExp$X <- NULL
keep <- filterdExp$Samples
seqtab.nochim <- readRDS(file.path(reportDir, "seqtab_nochim.RDS"))
## Keep qualify samples only
seqtab_fin <- seqtab.nochim[rownames(seqtab.nochim) %in% keep, ]
seqtabFinFile <- file.path(reportDir, paste0(projectn, "_seqtab_fin.RDS"))
saveRDS(seqtab_fin, seqtabFinFile)
```


```{r end1}

end_time <- Sys.time()
end_time - start_time
```
